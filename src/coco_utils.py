import copy
import os
from collections import Counter

import torch
import torch.utils.data
import torchvision
import transforms as T
from pycocotools import mask as coco_mask
from pycocotools.coco import COCO
from torch.utils.data import Dataset
from PIL import Image
import numpy as np

class FilterAndRemapCocoCategories:
    def __init__(self, categories, remap=True):
        self.categories = categories
        self.remap = remap

    def __call__(self, image, target):
        anno = target["annotations"]
        anno = [obj for obj in anno if obj["category_id"] in self.categories]
        if not self.remap:
            target["annotations"] = anno
            return image, target
        anno = copy.deepcopy(anno)
        for obj in anno:
            obj["category_id"] = self.categories.index(obj["category_id"])
        target["annotations"] = anno
        return image, target


def convert_coco_poly_to_mask(segmentations, height, width):
    masks = []
    for polygons in segmentations:
        rles = coco_mask.frPyObjects(polygons, height, width)
        mask = coco_mask.decode(rles)
        if len(mask.shape) < 3:
            mask = mask[..., None]
        mask = torch.as_tensor(mask, dtype=torch.uint8)
        mask = mask.any(dim=2)
        masks.append(mask)
    if masks:
        masks = torch.stack(masks, dim=0)
    else:
        masks = torch.zeros((0, height, width), dtype=torch.uint8)
    return masks


class ConvertCocoPolysToMask:
    def __init__(self, use_score):
        self.use_score = use_score

    def __call__(self, image, target):
        w, h = image.size

        image_id = target["image_id"]
        image_id = torch.tensor([image_id])

        anno = target["annotations"]

        anno = [obj for obj in anno if obj["iscrowd"] == 0]

        boxes = [obj["bbox"] for obj in anno]
        # guard against no boxes via resizing
        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)
        boxes[:, 2:] += boxes[:, :2]
        boxes[:, 0::2].clamp_(min=0, max=w)
        boxes[:, 1::2].clamp_(min=0, max=h)

        classes = [obj["category_id"] for obj in anno]
        classes = torch.tensor(classes, dtype=torch.int64)

        # segmentations = [obj["segmentation"] for obj in anno]
        # masks = convert_coco_poly_to_mask(segmentations, h, w)

        if self.use_score:
            scores = torch.tensor([obj["score"] if "score" in obj else 1 for obj in anno], dtype=torch.float32)
        else:
            scores = torch.tensor([1 for _ in anno], dtype=torch.float32)
        labeled = torch.tensor([obj["labeled"] if "labeled" in obj else 1 for obj in anno], dtype=torch.int8)

        keypoints = None
        if anno and "keypoints" in anno[0]:
            keypoints = [obj["keypoints"] for obj in anno]
            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)
            num_keypoints = keypoints.shape[0]
            if num_keypoints:
                keypoints = keypoints.view(num_keypoints, -1, 3)

        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])
        boxes = boxes[keep]
        classes = classes[keep]
        scores = scores[keep]
        labeled = labeled[keep]
        # masks = masks[keep]
        if keypoints is not None:
            keypoints = keypoints[keep]

        target = {}
        target["boxes"] = boxes
        target["labels"] = classes
        target["scores"] = scores
        target["labeled"] = labeled
        # target["masks"] = masks
        target["image_id"] = image_id
        if keypoints is not None:
            target["keypoints"] = keypoints

        # for conversion to coco api
        area = torch.tensor([obj["area"] for obj in anno])
        iscrowd = torch.tensor([obj["iscrowd"] for obj in anno])
        target["area"] = area
        target["iscrowd"] = iscrowd

        return image, target


def _coco_remove_images_without_annotations(dataset, cat_list=None):
    def _has_only_empty_bbox(anno):
        return all(any(o <= 1 for o in obj["bbox"][2:]) for obj in anno)

    def _count_visible_keypoints(anno):
        return sum(sum(1 for v in ann["keypoints"][2::3] if v > 0) for ann in anno)

    min_keypoints_per_image = 10

    def _has_valid_annotation(anno):
        # if it's empty, there is no annotation
        if len(anno) == 0:
            return False
        # if all boxes have close to zero area, there is no annotation
        if _has_only_empty_bbox(anno):
            return False
        # keypoints task have a slight different criteria for considering
        # if an annotation is valid
        if "keypoints" not in anno[0]:
            return True
        # for keypoint detection tasks, only consider valid images those
        # containing at least min_keypoints_per_image
        if _count_visible_keypoints(anno) >= min_keypoints_per_image:
            return True
        return False

    if not isinstance(dataset, torchvision.datasets.CocoDetection):
        raise TypeError(
            f"This function expects dataset of type torchvision.datasets.CocoDetection, instead  got {type(dataset)}"
        )
    ids = []
    for ds_idx, img_id in enumerate(dataset.ids):
        ann_ids = dataset.coco.getAnnIds(imgIds=img_id, iscrowd=None)
        anno = dataset.coco.loadAnns(ann_ids)
        if cat_list:
            anno = [obj for obj in anno if obj["category_id"] in cat_list]
        if _has_valid_annotation(anno):
            ids.append(ds_idx)

    dataset = torch.utils.data.Subset(dataset, ids)
    return dataset


def convert_to_coco_api(ds):
    coco_ds = COCO()
    # annotation IDs need to start at 1, not 0, see torchvision issue #1530
    ann_id = 1
    dataset = {"images": [], "categories": [], "annotations": []}
    categories = set()
    for img_idx in range(len(ds)):
        # find better way to get target
        # targets = ds.get_annotations(img_idx)
        img, targets = ds[img_idx]
        image_id = targets["image_id"].item()
        img_dict = {}
        img_dict["id"] = image_id
        img_dict["height"] = img.shape[-2]
        img_dict["width"] = img.shape[-1]
        dataset["images"].append(img_dict)
        bboxes = targets["boxes"].clone()
        bboxes[:, 2:] -= bboxes[:, :2]
        bboxes = bboxes.tolist()
        labels = targets["labels"].tolist()
        areas = targets["area"].tolist()
        iscrowd = targets["iscrowd"].tolist()
        if "masks" in targets:
            masks = targets["masks"]
            # make masks Fortran contiguous for coco_mask
            masks = masks.permute(0, 2, 1).contiguous().permute(0, 2, 1)
        if "keypoints" in targets:
            keypoints = targets["keypoints"]
            keypoints = keypoints.reshape(keypoints.shape[0], -1).tolist()
        num_objs = len(bboxes)
        for i in range(num_objs):
            ann = {}
            ann["image_id"] = image_id
            ann["bbox"] = bboxes[i]
            ann["category_id"] = labels[i]
            categories.add(labels[i])
            ann["area"] = areas[i]
            ann["iscrowd"] = iscrowd[i]
            ann["id"] = ann_id
            if "masks" in targets:
                ann["segmentation"] = coco_mask.encode(masks[i].numpy())
            if "keypoints" in targets:
                ann["keypoints"] = keypoints[i]
                ann["num_keypoints"] = sum(k != 0 for k in keypoints[i][2::3])
            dataset["annotations"].append(ann)
            ann_id += 1
    dataset["categories"] = [{"id": i} for i in sorted(categories)]
    coco_ds.dataset = dataset
    coco_ds.createIndex()
    return coco_ds


def get_coco_api_from_dataset(dataset):
    return convert_to_coco_api(dataset)


class CocoDetection(torchvision.datasets.CocoDetection):
    def __init__(self, img_folder, ann_file, transforms, use_score):
        super().__init__(img_folder, ann_file)
        self._transforms = transforms
        self.ann_file = ann_file
        self.mapping, self.inverse_coco = self.map_coco()
        self.use_score = use_score
        self.thresholds = self.get_thresholds()
    
    def get_thresholds(self,):
        th = [0.]
        for c in self.coco.dataset["categories"]:
            if "threshold" in c and self.use_score:
                th.append(c["threshold"])
            else:
                th.append(0)
        return th

    def map_coco(self,):
        mapping_coco = {}
        inverse_coco = {}
        for idx, c in enumerate(self.coco.cats):
            mapping_coco[c] = idx+1
            inverse_coco[idx+1] = c
        return mapping_coco, inverse_coco

    def __getitem__(self, idx):
        img, target = super().__getitem__(idx)
        img = img.resize((640,640))
        # img, target = self.images[idx], self.annots[idx]
        target = copy.deepcopy(target)
        for t in target:
            t["category_id"] = self.mapping[t["category_id"]]
        image_id = self.ids[idx]
        target = dict(image_id=image_id, annotations=target)
        if self._transforms is not None:
            img, target = self._transforms(img, target)
        return img, target

class CustomDataset(Dataset):
    def __init__(self, data, transforms, train=False):
        self._transforms = transforms
        self.train = train
        self.images = self._load_data(data)

    def _load_data(self,file):
        images = []
        with open(file, 'r') as f:
            for line in f:
                images.append(line.split()[0])     
        return images

    def nroi_at(self,i):
        im_path = self.images[i]
        im    =  Image.open(im_path)
        name  =  im_path.split('/')[-1]
        return im, name    

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        im_data, name = self.nroi_at(idx)
        # w, h
        im_info_orig = torch.FloatTensor([im_data.size[0], im_data.size[1]])
        im_data = im_data.resize((640,640))
        im_info_resize = torch.FloatTensor([im_data.size[0], im_data.size[1]])
        # img, target = self.images[idx], self.annots[idx]
        # target = []
        # target = copy.deepcopy(target)
        # for t in target:
        #     t["category_id"] = self.mapping[t["category_id"]]
        # image_id = self.ids[idx]
        # target = dict(image_id=image_id, annotations=target)
        if self._transforms is not None:
            im_data = self._transforms(im_data, target = None)
        return im_data, name, im_info_orig, im_info_resize


def get_coco(root, image_set, transforms, ann_file, dataset, use_score):

    t = [ConvertCocoPolysToMask(use_score)]

    if transforms is not None:
        t.append(transforms)
    transforms = T.Compose(t)

    if dataset == "coco":
        if image_set != "eval":
            ann_file = ann_file
            img_folder = os.path.join(root, "train2017")
        else:
            ann_file = "/home/zafar/old_pc/data_sets/coco2017/annotations/instances_val2017.json"
            img_folder = os.path.join(root, "val2017")
    elif dataset == "dior":
        img_folder = os.path.join(root, "all_images")
        if image_set == "eval":
            ann_file = "dior/annotations/eval_annotations.json"   
    elif dataset == "custom":
        image_folder = root
    dataset = CocoDetection(img_folder, ann_file, transforms=transforms, use_score=use_score)

    if image_set == "train":
        dataset = _coco_remove_images_without_annotations(dataset)

    return dataset
